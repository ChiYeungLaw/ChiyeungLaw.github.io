<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research - Ziyang's Page</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="icon" type="image/jpeg" href="images/me.jpg">
</head>
<body>
    <nav class="navbar">
        <div class="nav-container">
            <div class="nav-brand">Ziyang's Page</div>
            <div class="nav-links">
                <a href="index.html" class="nav-link">Home</a>
                <a href="research.html" class="nav-link active">Research</a>
                <a href="contact.html" class="nav-link">Contact</a>
            </div>
        </div>
    </nav>

    <main class="main-content">
        <div class="container">
            <!-- <h1 class="page-title">Research</h1> -->
            <div class="research-content">
                <section class="research-section">
                    <h2>Selected Publications (* equal contribution)</h2>
                    <div class="publications-list">
                        <div class="year-section">
                            <h3>Preprints</h3>
                            <div class="publication-item">
                                <h4>Aria-UI: Visual Grounding for GUI Instructions</h4>
                                <p class="authors">Yuhao Yang, Yue Wang, Dongxu Li, Ziyang Luo, Bei Chen, Chao Huang, and Junnan Li</p>
                                <div class="links">
                                    <a href="https://arxiv.org/abs/2412.16256" target="_blank" rel="noopener noreferrer">Paper</a>
                                </div>
                            </div>
                        </div>

                        <div class="year-section">
                            <h3>2025</h3>
                            <div class="publication-item">
                                <h4>VideoAutoArena: An Automated Arena for Evaluating Large Multimodal Models in Video Analysis through User Simulation</h4>
                                <p class="venue">CVPR 2025</p>
                                <p class="authors">Ziyang Luo, Haoning Wu, Dongxu Li, Jing Ma, Mohan Kankanhalli, and Junnan Li</p>
                                <div class="links">
                                    <a href="https://arxiv.org/pdf/2411.13281" target="_blank" rel="noopener noreferrer">Paper</a>
                                    <a href="https://videoautoarena.github.io/" target="_blank" rel="noopener noreferrer">Project Page</a>
                                </div>
                            </div>

                            <div class="publication-item">
                                <h4>ScreenSpot-Pro: GUI Grounding for Professional High-Resolution Computer Use</h4>
                                <p class="venue">ICLR 2025 Workshop</p>
                                <p class="authors">Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang and Tat-Seng Chua</p>
                                <div class="links">
                                    <a href="https://likaixin2000.github.io/papers/ScreenSpot_Pro.pdf" target="_blank" rel="noopener noreferrer">Paper</a>
                                    <a href="https://gui-agent.github.io/grounding-leaderboard/" target="_blank" rel="noopener noreferrer">Project Page</a>
                                </div>
                            </div>

                            <div class="publication-item">
                                <h4>ScratchEval: Are GPT-4o Smarter than My Child? Evaluating Large Multimodal Models with Visual Programming Challenges</h4>
                                <p class="venue">NAACL 2025</p>
                                <p class="authors">Rao Fu, Ziyang Luo, Hongzhan Lin, Zhen Ye, Jing Ma</p>
                                <div class="links">
                                    <a href="https://arxiv.org/abs/2411.18932" target="_blank" rel="noopener noreferrer">Paper</a>
                                    <a href="https://github.com/HKBUNLP/ScratchEval" target="_blank" rel="noopener noreferrer">Code</a>
                                </div>
                            </div>

                            <div class="publication-item">
                                <h4>CodeHalu: Code Hallucinations in LLMs Driven by Execution-based Verification</h4>
                                <p class="venue">AAAI 2025</p>
                                <p class="authors">Yuchen Tian, Weixiang Yan, Qian Yang, Xuandong Zhao, Qian Chen, Wen Wang, Ziyang Luo, Lei Ma, Dawn Song</p>
                                <div class="links">
                                    <a href="https://arxiv.org/abs/2405.00253" target="_blank" rel="noopener noreferrer">Paper</a>
                                    <a href="https://github.com/yuchen814/CodeHalu" target="_blank" rel="noopener noreferrer">Code</a>
                                </div>
                            </div>

                            <div class="publication-item">
                                <h4>CodeJudge-Eval: Can Large Language Models be Good Judges in Code Understanding?</h4>
                                <p class="venue">COLING 2025</p>
                                <p class="authors">Yuwei Zhao*, Ziyang Luo*, Yuchen Tian, Hongzhan Lin, Weixiang Yan, Annan Li, and Jing Ma</p>
                                <div class="links">
                                    <a href="https://arxiv.org/abs/2408.10718" target="_blank" rel="noopener noreferrer">Paper</a>
                                    <a href="https://huggingface.co/datasets/CodeResearch/CodeJudge-Eval" target="_blank" rel="noopener noreferrer">Data</a>
                                </div>
                            </div>

                            <div class="publication-item">
                                <h4>MFC-Bench: Benchmarking Multimodal Fact-Checking with Large Vision-Language Models</h4>
                                <p class="venue">ICLR 2025 Workshop</p>
                                <p class="authors">Shengkang Wang*, Hongzhan Lin*, Ziyang Luo*, Zhen Ye, Guang Chen, and Jing Ma</p>
                                <div class="links">
                                    <a href="https://arxiv.org/pdf/2406.11288" target="_blank" rel="noopener noreferrer">Paper</a>
                                    <a href="https://github.com/wskbest/MFC-Bench" target="_blank" rel="noopener noreferrer">Data</a>
                                </div>
                            </div>
                        </div>

                        <div class="year-section">
                            <h3>2024</h3>
                            <div class="publication-item">
                                <h4>WizardCoder: Empowering Code Large Language Models with Evol-Instruct</h4>
                                <p class="venue">ICLR 2024</p>
                                <p class="authors">Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang</p>
                                <div class="links">
                                    <a href="https://arxiv.org/abs/2306.08568" target="_blank" rel="noopener noreferrer">Paper</a>
                                    <a href="https://huggingface.co/WizardLM/WizardCoder-15B-V1.0" target="_blank" rel="noopener noreferrer">Model</a>
                                </div>
                            </div>

                            <div class="publication-item">
                                <h4>AMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation</h4>
                                <p class="venue">EMNLP 2024</p>
                                <p class="authors">Ziyang Luo, Xin Li, Hongzhan Lin, Jing Ma, and Lidong Bing</p>
                                <div class="links">
                                    <a href="https://arxiv.org/abs/2410.00558" target="_blank" rel="noopener noreferrer">Paper</a>
                                </div>
                            </div>

                            <div class="publication-item">
                                <h4>Towards Low-Resource Harmful Meme Detection with LMM Agents</h4>
                                <p class="venue">EMNLP 2024</p>
                                <p class="authors">Jianzhao Huang, Hongzhan Lin, Ziyan Liu, Ziyang Luo, Guang Chen, and Jing Ma</p>
                                <div class="links">
                                    <a href="https://aclanthology.org/2024.emnlp-main.136/" target="_blank" rel="noopener noreferrer">Paper</a>
                                </div>
                            </div>

                            <div class="publication-item">
                                <h4>MMCode: Evaluating Multi-Modal Code Large Language Models with Visually Rich Programming Problems</h4>
                                <p class="venue">EMNLP 2024</p>
                                <p class="authors">Kaixin Li, Yuchen Tian, Qisheng Hu, Ziyang Luo, and Jing Ma</p>
                                <div class="links">
                                    <a href="https://arxiv.org/pdf/2404.09486.pdf" target="_blank" rel="noopener noreferrer">Paper</a>
                                    <a href="https://huggingface.co/datasets/likaixin/MMCode" target="_blank" rel="noopener noreferrer">Data</a>
                                </div>
                            </div>

                            <div class="publication-item">
                                <h4>CofiPara: A Coarse-to-fine Paradigm for Multimodal Sarcasm Target Identification with Large Multimodal Models</h4>
                                <p class="venue">ACL 2024</p>
                                <p class="authors">Zixin Chen, Hongzhan Lin, Ziyang Luo, Mingfei Cheng, Jing Ma, and Guang Chen</p>
                                <div class="links">
                                    <a href="https://arxiv.org/abs/2405.00390" target="_blank" rel="noopener noreferrer">Paper</a>
                                    <a href="https://github.com/Lbotirx/CofiPara" target="_blank" rel="noopener noreferrer">Code</a>
                                </div>
                            </div>

                            <div class="publication-item">
                                <h4>Towards Explainable Harmful Meme Detection through Multimodal Debate between Large Language Models</h4>
                                <p class="venue">WWW 2024</p>
                                <p class="authors">Hongzhan Lin, Ziyang Luo, Wei Gao, Jing Ma, Bo Wang, and Ruichao Yang</p>
                                <div class="links">
                                    <a href="https://arxiv.org/abs/2401.13298" target="_blank" rel="noopener noreferrer">Paper</a>
                                    <a href="https://github.com/HKBUNLP/ExplainHM-WWW2024" target="_blank" rel="noopener noreferrer">Code</a>
                                </div>
                            </div>
                        </div>

                        <div class="year-section">
                            <h3>2023</h3>
                            <div class="publication-item">
                                <h4>LexLIP: Lexicon-Bottlenecked Language-Image Pre-Training for Large-Scale Image-Text Sparse Retrieval</h4>
                                <p class="venue">ICCV 2023</p>
                                <p class="authors">Ziyang Luo, Pu Zhao, Can Xu, Xiubo Geng, Tao Shen, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang</p>
                                <div class="links">
                                    <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Luo_LexLIP_Lexicon-Bottlenecked_Language-Image_Pre-Training_for_Large-Scale_Image-Text_Sparse_Retrieval_ICCV_2023_paper.pdf" target="_blank" rel="noopener noreferrer">Paper</a>
                                    <a href="https://github.com/ChiYeungLaw/LexLIP-ICCV23" target="_blank" rel="noopener noreferrer">Code</a>
                                </div>
                            </div>

                            <div class="publication-item">
                                <h4>Beneath the Surface: Unveiling Harmful Memes with Multimodal Reasoning Distilled from Large Language Models</h4>
                                <p class="venue">EMNLP 2023</p>
                                <p class="authors">Hongzhan Lin*, Ziyang Luo*, Jing Ma, and Long Chen</p>
                                <div class="links">
                                    <a href="https://aclanthology.org/2023.findings-emnlp.611/" target="_blank" rel="noopener noreferrer">Paper</a>
                                    <a href="https://github.com/HKBUNLP/Mr.Harm-EMNLP2023" target="_blank" rel="noopener noreferrer">Code</a>
                                </div>
                            </div>

                            <div class="publication-item">
                                <h4>Zero-Shot Rumor Detection with Propagation Structure via Prompt Learning</h4>
                                <p class="venue">AAAI 2023</p>
                                <p class="authors">Hongzhan Lin, Pengyao Yi, Jing Ma, Haiyun Jiang, Ziyang Luo, Shuming Shi, Ruifang Liu</p>
                                <div class="links">
                                    <a href="https://arxiv.org/abs/2212.01117" target="_blank" rel="noopener noreferrer">Paper</a>
                                </div>
                            </div>
                        </div>

                        <div class="year-section">
                            <h3>2022</h3>
                            <div class="publication-item">
                                <h4>A Coarse-to-fine Cascaded Evidence-Distillation Neural Network for Explainable Fake News Detection</h4>
                                <p class="venue">COLING 2022</p>
                                <p class="authors">Zhiwei Yang, Jing Ma, Hechang Chen, Hongzhan Lin, Ziyang Luo, Yi Chang</p>
                                <div class="links">
                                    <a href="https://aclanthology.org/2022.coling-1.230/" target="_blank" rel="noopener noreferrer">Paper</a>
                                </div>
                            </div>

                            <div class="publication-item">
                                <h4>Conditioned Masked Language and Image Modeling for Image-Text Dense Retrieval</h4>
                                <p class="venue">EMNLP 2022</p>
                                <p class="authors">Ziyang Luo, Yadong Xi, Rongsheng Zhang, Gongzheng Li, Zeng Zhao, and Jing Ma</p>
                                <div class="links">
                                    <a href="https://aclanthology.org/2022.findings-emnlp.10/" target="_blank" rel="noopener noreferrer">Paper</a>
                                </div>
                            </div>

                            <div class="publication-item">
                                <h4>DecBERT: Enhancing the Language Understanding of BERT with Causal Attention Masks</h4>
                                <p class="venue">NAACL 2022</p>
                                <p class="authors">Ziyang Luo, Yadong Xi, Jing Ma, Zhiwei Yang, Xiaoxi Mao, Changjie Fan, Rongsheng Zhang</p>
                                <div class="links">
                                    <a href="https://aclanthology.org/2022.findings-naacl.89/" target="_blank" rel="noopener noreferrer">Paper</a>
                                </div>
                            </div>

                            <div class="publication-item">
                                <h4>Easy and Efficient Transformer: Scalable Inference Solution for Large NLP Model</h4>
                                <p class="venue">NAACL 2022</p>
                                <p class="authors">Yadong Xi, Gongzheng Li, Jingzhen Ding, Duan Wang, Ziyang Luo, Rongsheng Zhang, Bai Liu, Changjie Fan, Xiaoxi Mao, Zeng Zhao</p>
                                <div class="links">
                                    <a href="https://aclanthology.org/2022.naacl-industry.8/" target="_blank" rel="noopener noreferrer">Paper</a>
                                </div>
                            </div>
                        </div>

                        <div class="year-section">
                            <h3>2021</h3>
                            <div class="publication-item">
                                <h4>Positional Artefacts Propagate Through Masked Language Model Embeddings</h4>
                                <p class="venue">ACL 2021</p>
                                <p class="authors">Ziyang Luo, Artur Kulmizev, and Xiaoxi Mao</p>
                                <div class="links">
                                    <a href="https://aclanthology.org/2021.acl-long.413/" target="_blank" rel="noopener noreferrer">Paper</a>
                                </div>
                            </div>

                            <div class="publication-item">
                                <h4>Smoothing with Fake Label</h4>
                                <p class="venue">CIKM 2021</p>
                                <p class="authors">Ziyang Luo, Yadong Xi, and Xiaoxi Mao</p>
                                <div class="links">
                                    <a href="/research/Fake_Label_CIKM.pdf" target="_blank" rel="noopener noreferrer">Paper</a>
                                </div>
                            </div>

                            <div class="publication-item">
                                <h4>Gender Bias Hidden Behind Chinese Word Embeddings: The Case of Chinese Adjectives</h4>
                                <p class="venue">GeBNLP 2021</p>
                                <p class="authors">Meichun Jiao, Ziyang Luo</p>
                                <div class="links">
                                    <a href="https://aclanthology.org/2021.gebnlp-1.2/" target="_blank" rel="noopener noreferrer">Paper</a>
                                </div>
                            </div>

                            <div class="publication-item">
                                <h4>Have Attention Heads in BERT Learned Constituency Grammar?</h4>
                                <p class="venue">EACL 2021 SRW</p>
                                <p class="authors">Ziyang Luo</p>
                                <div class="links">
                                    <a href="https://aclanthology.org/2021.eacl-srw.2/" target="_blank" rel="noopener noreferrer">Paper</a>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section class="research-section">
                    <h2>Selected Open-Source Projects</h2>
                    <div class="publications-list">
                        <div class="publication-item">
                            <h4>VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs</h4>
                            <p class="authors">VideoLLaMA Team@Alibaba</p>
                            <div class="links">
                                <a href="https://arxiv.org/pdf/2406.07476" target="_blank" rel="noopener noreferrer">Paper</a>
                                <a href="https://github.com/DAMO-NLP-SG/VideoLLaMA2" target="_blank" rel="noopener noreferrer">Code</a>
                            </div>
                        </div>

                        <div class="publication-item">
                            <h4>WizardCoder: Empowering Code Large Language Models with Evol-Instruct</h4>
                            <p class="authors">WizardLM Team@Microsoft</p>
                            <div class="links">
                                <a href="https://arxiv.org/abs/2306.08568" target="_blank" rel="noopener noreferrer">Paper</a>
                                <a href="https://github.com/nlpxucan/WizardLM" target="_blank" rel="noopener noreferrer">Code</a>
                            </div>
                        </div>

                        <div class="publication-item">
                            <h4>AURORA-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order</h4>
                            <p class="authors">Aurora-M Open-Source Community</p>
                            <div class="links">
                                <a href="https://arxiv.org/pdf/2404.00399.pdf" target="_blank" rel="noopener noreferrer">Paper</a>
                                <a href="https://huggingface.co/collections/aurora-m/aurora-m-models-65fdfdff62471e09812f5407" target="_blank" rel="noopener noreferrer">Models</a>
                            </div>
                        </div>
                    </div>
                </section>

                <section class="research-section">
                    <h2>Master Thesis</h2>
                    <div class="publications-list">
                        <div class="publication-item">
                            <h4>Analyzing the Anisotropy Phenomenon in Transformer-based Masked Language Models</h4>
                            <p class="authors">Supervised by <a href="https://scholar.google.nl/citations?hl=en&user=Cgg6_W0AAAAJ&view_op=list_works&sortby=pubdate" target="_blank" rel="noopener noreferrer">Artur Kulmizev</a>, Uppsala University, 2021</p>
                            <div class="links">
                                <a href="http://uu.diva-portal.org/smash/record.jsf?pid=diva2%3A1565827&dswid=61" target="_blank" rel="noopener noreferrer">Thesis</a>
                            </div>
                        </div>
                    </div>
                </section>
            </div>
        </div>
    </main>
</body>
</html>