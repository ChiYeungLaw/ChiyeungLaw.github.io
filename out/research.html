<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/a34f9d1faa5f3315-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/31825d868cc34e7f.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-d6abbf1fcfdef890.js"/><script src="/_next/static/chunks/4bd1b696-6e2a7fb160913143.js" async=""></script><script src="/_next/static/chunks/684-3ba6d0b3a93b1564.js" async=""></script><script src="/_next/static/chunks/main-app-e0f944c77adcee76.js" async=""></script><script src="/_next/static/chunks/app/layout-21c00bc2bfe80b4e.js" async=""></script><meta name="next-size-adjust" content=""/><title>Ziyang&amp;apos;s Page</title><meta name="description" content="Welcome to Ziyang Luo&#x27;s personal website"/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="16x16"/><link rel="icon" href="/images/me.jpg"/><link rel="apple-touch-icon" href="/images/me.jpg"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__className_d65c78"><nav class="bg-gray-800 text-white"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex items-center justify-between h-16"><div class="flex items-center"><div class="flex-shrink-0"><span class="text-xl font-bold">Ziyang&#x27;s Page</span></div><div class="hidden md:block"><div class="ml-10 flex items-baseline space-x-4"><a class="px-3 py-2 rounded-md text-sm font-medium hover:bg-gray-700" href="/">Home</a><a class="px-3 py-2 rounded-md text-sm font-medium hover:bg-gray-700" href="/research">Research</a><a class="px-3 py-2 rounded-md text-sm font-medium hover:bg-gray-700" href="/contact">Contact</a></div></div></div></div></div></nav><main class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8"><div class="space-y-12"><section><h1 class="text-3xl font-bold mb-6 text-white">Selected Publications</h1><p class="text-gray-200 mb-4">(* equal contribution)</p><div class="space-y-8"><div><h2 class="text-xl font-semibold mb-4 text-white">Preprints</h2><div class="space-y-6"><div class="bg-gray-800/50 p-6 rounded-lg"><p class="text-gray-200 mb-2"><span class="font-semibold">Aria-UI: Visual Grounding for GUI Instructions</span><br/><span class="text-gray-300">Yuhao Yang, Yue Wang, Dongxu Li, Ziyang Luo, Bei Chen, Chao Huang, and Junnan Li</span><br/><a href="https://arxiv.org/abs/2412.16256" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Paper</a></p></div></div></div><div><h2 class="text-xl font-semibold mb-4 text-white">2025</h2><div class="space-y-6"><div class="bg-gray-800/50 p-6 rounded-lg"><p class="text-gray-200 mb-2"><span class="font-semibold">VideoAutoArena: An Automated Arena for Evaluating Large Multimodal Models in Video Analysis through User Simulation</span><br/><strong>CVPR 2025</strong>: Proceedings of the 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition<br/><span class="text-gray-300">Ziyang Luo, Haoning Wu, Dongxu Li, Jing Ma, Mohan Kankanhalli, and Junnan Li</span><br/><a href="https://arxiv.org/pdf/2411.13281" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Paper</a> | <a href="https://videoautoarena.github.io/" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Project Page</a></p></div><div class="bg-gray-800/50 p-6 rounded-lg"><p class="text-gray-200 mb-2"><span class="font-semibold">ScreenSpot-Pro: GUI Grounding for Professional High-Resolution Computer Use</span><br/><strong>ICLR 2025 Workshop</strong>: Reasoning and Planning for LLMs Workshop<br/><span class="text-gray-300">Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang and Tat-Seng Chua</span><br/><a href="https://likaixin2000.github.io/papers/ScreenSpot_Pro.pdf" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Paper</a> | <a href="https://gui-agent.github.io/grounding-leaderboard/" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Project Page</a></p></div><div class="bg-gray-800/50 p-6 rounded-lg"><p class="text-gray-200 mb-2"><span class="font-semibold">ScratchEval: Are GPT-4o Smarter than My Child? Evaluating Large Multimodal Models with Visual Programming Challenges</span><br/><strong>NAACL 2025</strong>: Proceedings of the 2025 Annual Conference of the North American Chapter of the Association for Computational Linguistics<br/><span class="text-gray-300">Rao Fu, Ziyang Luo, Hongzhan Lin, Zhen Ye, Jing Ma</span><br/><a href="https://arxiv.org/abs/2411.18932" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Paper</a> | <a href="https://github.com/HKBUNLP/ScratchEval" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Code</a></p></div><div class="bg-gray-800/50 p-6 rounded-lg"><p class="text-gray-200 mb-2"><span class="font-semibold">CodeHalu: Code Hallucinations in LLMs Driven by Execution-based Verification</span><br/><strong>AAAI 2025</strong>: Proceedings of the 39th Annual AAAI Conference on Artificial Intelligence<br/><span class="text-gray-300">Yuchen Tian, Weixiang Yan, Qian Yang, Xuandong Zhao, Qian Chen, Wen Wang, Ziyang Luo, Lei Ma, Dawn Song</span><br/><a href="https://arxiv.org/abs/2405.00253" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Paper</a> | <a href="https://github.com/yuchen814/CodeHalu" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Code</a></p></div><div class="bg-gray-800/50 p-6 rounded-lg"><p class="text-gray-200 mb-2"><span class="font-semibold">CodeJudge-Eval: Can Large Language Models be Good Judges in Code Understanding?</span><br/><strong>COLING 2025</strong>: Proceedings of the 31st International Conference on Computational Linguistics<br/><span class="text-gray-300">Yuwei Zhao*, Ziyang Luo*, Yuchen Tian, Hongzhan Lin, Weixiang Yan, Annan Li, and Jing Ma</span><br/><a href="https://arxiv.org/abs/2408.10718" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Paper</a> | <a href="https://huggingface.co/datasets/CodeResearch/CodeJudge-Eval" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Data</a></p></div><div class="bg-gray-800/50 p-6 rounded-lg"><p class="text-gray-200 mb-2"><span class="font-semibold">MFC-Bench: Benchmarking Multimodal Fact-Checking with Large Vision-Language Models</span><br/><strong>ICLR 2025 Workshop</strong>: Reasoning and Planning for LLMs Workshop<br/><span class="text-gray-300">Shengkang Wang*, Hongzhan Lin*, Ziyang Luo*, Zhen Ye, Guang Chen, and Jing Ma</span><br/><a href="https://arxiv.org/pdf/2406.11288" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Paper</a> | <a href="https://github.com/wskbest/MFC-Bench" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Data</a></p></div></div></div><div><h2 class="text-xl font-semibold mb-4 text-white">2024</h2><div class="space-y-6"><div class="bg-gray-800/50 p-6 rounded-lg"><p class="text-gray-200 mb-2"><span class="font-semibold">WizardCoder: Empowering Code Large Language Models with Evol-Instruct</span><br/><strong>ICLR 2024</strong>: Proceedings of the Twelfth International Conference on Learning Representations<br/><span class="text-gray-300">Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang</span><br/><a href="https://arxiv.org/abs/2306.08568" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Paper</a> | <a href="https://huggingface.co/WizardLM/WizardCoder-15B-V1.0" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Model</a></p></div><div class="bg-gray-800/50 p-6 rounded-lg"><p class="text-gray-200 mb-2"><span class="font-semibold">AMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation</span><br/><strong>EMNLP 2024</strong>: Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing<br/><span class="text-gray-300">Ziyang Luo, Xin Li, Hongzhan Lin, Jing Ma, and Lidong Bing</span><br/><a href="https://arxiv.org/abs/2410.00558" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Paper</a></p></div><div class="bg-gray-800/50 p-6 rounded-lg"><p class="text-gray-200 mb-2"><span class="font-semibold">Towards Low-Resource Harmful Meme Detection with LMM Agents</span><br/><strong>EMNLP 2024</strong>: Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing<br/><span class="text-gray-300">Jianzhao Huang, Hongzhan Lin, Ziyan Liu, Ziyang Luo, Guang Chen, and Jing Ma</span><br/><a href="https://aclanthology.org/2024.emnlp-main.136/" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Paper</a></p></div><div class="bg-gray-800/50 p-6 rounded-lg"><p class="text-gray-200 mb-2"><span class="font-semibold">MMCode: Evaluating Multi-Modal Code Large Language Models with Visually Rich Programming Problems</span><br/><strong>EMNLP 2024</strong>: Findings of the 2024 Conference on Empirical Methods in Natural Language Processing<br/><span class="text-gray-300">Kaixin Li, Yuchen Tian, Qisheng Hu, Ziyang Luo, and Jing Ma</span><br/><a href="https://arxiv.org/pdf/2404.09486.pdf" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Paper</a> | <a href="https://huggingface.co/datasets/likaixin/MMCode" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Data</a></p></div><div class="bg-gray-800/50 p-6 rounded-lg"><p class="text-gray-200 mb-2"><span class="font-semibold">CofiPara: A Coarse-to-fine Paradigm for Multimodal Sarcasm Target Identification with Large Multimodal Models</span><br/><strong>ACL 2024</strong>: Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics<br/><span class="text-gray-300">Zixin Chen, Hongzhan Lin, Ziyang Luo, Mingfei Cheng, Jing Ma, and Guang Chen</span><br/><a href="https://arxiv.org/abs/2405.00390" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Paper</a> | <a href="https://github.com/Lbotirx/CofiPara" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Code</a></p></div><div class="bg-gray-800/50 p-6 rounded-lg"><p class="text-gray-200 mb-2"><span class="font-semibold">Towards Explainable Harmful Meme Detection through Multimodal Debate between Large Language Models</span><br/><strong>WWW 2024</strong>: The ACM Web Conference 2024<br/><span class="text-gray-300">Hongzhan Lin, Ziyang Luo, Wei Gao, Jing Ma, Bo Wang, and Ruichao Yang</span><br/><a href="https://arxiv.org/abs/2401.13298" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Paper</a> | <a href="https://github.com/HKBUNLP/ExplainHM-WWW2024" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Code</a></p></div></div></div><div><h2 class="text-xl font-semibold mb-4 text-white">2023</h2><div class="space-y-6"><div class="bg-gray-800/50 p-6 rounded-lg"><p class="text-gray-200 mb-2"><span class="font-semibold">LexLIP: Lexicon-Bottlenecked Language-Image Pre-Training for Large-Scale Image-Text Sparse Retrieval</span><br/><strong>ICCV 2023</strong>: Proceedings of the IEEE/CVF International Conference on Computer Vision<br/><span class="text-gray-300">Ziyang Luo, Pu Zhao, Can Xu, Xiubo Geng, Tao Shen, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang</span><br/><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Luo_LexLIP_Lexicon-Bottlenecked_Language-Image_Pre-Training_for_Large-Scale_Image-Text_Sparse_Retrieval_ICCV_2023_paper.pdf" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Paper</a> | <a href="https://github.com/ChiYeungLaw/LexLIP-ICCV23" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Code</a></p></div><div class="bg-gray-800/50 p-6 rounded-lg"><p class="text-gray-200 mb-2"><span class="font-semibold">Beneath the Surface: Unveiling Harmful Memes with Multimodal Reasoning Distilled from Large Language Models</span><br/><strong>EMNLP 2023</strong>: Findings of the 2023 Conference on Empirical Methods in Natural Language Processing<br/><span class="text-gray-300">Hongzhan Lin*, Ziyang Luo*, Jing Ma, and Long Chen</span><br/><a href="https://aclanthology.org/2023.findings-emnlp.611/" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Paper</a> | <a href="https://github.com/HKBUNLP/Mr.Harm-EMNLP2023" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Code</a></p></div><div class="bg-gray-800/50 p-6 rounded-lg"><p class="text-gray-200 mb-2"><span class="font-semibold">Zero-Shot Rumor Detection with Propagation Structure via Prompt Learning</span><br/><strong>AAAI 2023</strong>: Proceedings of the 2023 AAAI Conference on Artificial Intelligence<br/><span class="text-gray-300">Hongzhan Lin, Pengyao Yi, Jing Ma, Haiyun Jiang, Ziyang Luo, Shuming Shi, Ruifang Liu</span><br/><a href="https://arxiv.org/abs/2212.01117" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Paper</a></p></div></div></div><div><h2 class="text-xl font-semibold mb-4 text-white">2022</h2><div class="space-y-6"><div class="bg-gray-800/50 p-6 rounded-lg"><p class="text-gray-200 mb-2"><span class="font-semibold">A Coarse-to-fine Cascaded Evidence-Distillation Neural Network for Explainable Fake News Detection</span><br/><strong>COLING 2022</strong>: Proceedings of the 29th International Conference on Computational Linguistics<br/><span class="text-gray-300">Zhiwei Yang, Jing Ma, Hechang Chen, Hongzhan Lin, Ziyang Luo, Yi Chang</span><br/><a href="https://aclanthology.org/2022.coling-1.230/" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Paper</a></p></div><div class="bg-gray-800/50 p-6 rounded-lg"><p class="text-gray-200 mb-2"><span class="font-semibold">Conditioned Masked Language and Image Modeling for Image-Text Dense Retrieval</span><br/><strong>EMNLP 2022</strong>: Findings of the 2022 Conference on Empirical Methods in Natural Language Processing<br/><span class="text-gray-300">Ziyang Luo, Yadong Xi, Rongsheng Zhang, Gongzheng Li, Zeng Zhao, and Jing Ma</span><br/><a href="https://aclanthology.org/2022.findings-emnlp.10/" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Paper</a></p></div><div class="bg-gray-800/50 p-6 rounded-lg"><p class="text-gray-200 mb-2"><span class="font-semibold">DecBERT: Enhancing the Language Understanding of BERT with Causal Attention Masks</span><br/><strong>NAACL 2022</strong>: Findings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics<br/><span class="text-gray-300">Ziyang Luo, Yadong Xi, Jing Ma, Zhiwei Yang, Xiaoxi Mao, Changjie Fan, Rongsheng Zhang</span><br/><a href="https://aclanthology.org/2022.findings-naacl.89/" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Paper</a></p></div><div class="bg-gray-800/50 p-6 rounded-lg"><p class="text-gray-200 mb-2"><span class="font-semibold">Easy and Efficient Transformer: Scalable Inference Solution for Large NLP Model</span><br/><strong>NAACL 2022</strong>: Industry Track<br/><span class="text-gray-300">Yadong Xi, Gongzheng Li, Jingzhen Ding, Duan Wang, Ziyang Luo, Rongsheng Zhang, Bai Liu, Changjie Fan, Xiaoxi Mao, Zeng Zhao</span><br/><a href="https://aclanthology.org/2022.naacl-industry.8/" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Paper</a></p></div></div></div><div><h2 class="text-xl font-semibold mb-4 text-white">2021</h2><div class="space-y-6"><div class="bg-gray-800/50 p-6 rounded-lg"><p class="text-gray-200 mb-2"><span class="font-semibold">Positional Artefacts Propagate Through Masked Language Model Embeddings</span><br/><strong>ACL 2021</strong>: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics<br/><span class="text-gray-300">Ziyang Luo, Artur Kulmizev, and Xiaoxi Mao</span><br/><a href="https://aclanthology.org/2021.acl-long.413/" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Paper</a></p></div><div class="bg-gray-800/50 p-6 rounded-lg"><p class="text-gray-200 mb-2"><span class="font-semibold">Smoothing with Fake Label</span><br/><strong>CIKM 2021</strong>: Proceedings of the 30th ACM International Conference on Information and Knowledge Management<br/><span class="text-gray-300">Ziyang Luo, Yadong Xi, and Xiaoxi Mao</span><br/><a href="/research/Fake_Label_CIKM.pdf" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Paper</a></p></div><div class="bg-gray-800/50 p-6 rounded-lg"><p class="text-gray-200 mb-2"><span class="font-semibold">Gender Bias Hidden Behind Chinese Word Embeddings: The Case of Chinese Adjectives</span><br/><strong>GeBNLP 2021</strong>: Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing<br/><span class="text-gray-300">Meichun Jiao, Ziyang Luo</span><br/><a href="https://aclanthology.org/2021.gebnlp-1.2/" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Paper</a></p></div><div class="bg-gray-800/50 p-6 rounded-lg"><p class="text-gray-200 mb-2"><span class="font-semibold">Have Attention Heads in BERT Learned Constituency Grammar?</span><br/><strong>EACL 2021 SRW</strong>: Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop<br/><span class="text-gray-300">Ziyang Luo</span><br/><a href="https://aclanthology.org/2021.eacl-srw.2/" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Paper</a></p></div></div></div></div></section><section><h2 class="text-2xl font-bold mb-6 text-white">Selected Open-Source Projects</h2><div class="space-y-6"><div class="bg-gray-800/50 p-6 rounded-lg"><p class="text-gray-200"><span class="font-semibold">VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs</span><br/>VideoLLaMA Team@Alibaba<br/><a href="https://arxiv.org/pdf/2406.07476" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Paper</a> | <a href="https://github.com/DAMO-NLP-SG/VideoLLaMA2" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Code</a></p></div><div class="bg-gray-800/50 p-6 rounded-lg"><p class="text-gray-200"><span class="font-semibold">WizardCoder: Empowering Code Large Language Models with Evol-Instruct</span><br/>WizardLM Team@Microsoft<br/><a href="https://arxiv.org/abs/2306.08568" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Paper</a> | <a href="https://github.com/nlpxucan/WizardLM" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Code</a></p></div><div class="bg-gray-800/50 p-6 rounded-lg"><p class="text-gray-200"><span class="font-semibold">AURORA-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order</span><br/>Aurora-M Open-Source Community<br/><a href="https://arxiv.org/pdf/2404.00399.pdf" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Paper</a> | <a href="https://huggingface.co/collections/aurora-m/aurora-m-models-65fdfdff62471e09812f5407" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Models</a></p></div></div></section><section><h2 class="text-2xl font-bold mb-6 text-white">Master Thesis</h2><div class="bg-gray-800/50 p-6 rounded-lg"><p class="text-gray-200"><span class="font-semibold">Analyzing the Anisotropy Phenomenon in Transformer-based Masked Language Models</span><br/>Supervised by <a href="https://scholar.google.nl/citations?hl=en&amp;user=Cgg6_W0AAAAJ&amp;view_op=list_works&amp;sortby=pubdate" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Artur Kulmizev</a>, Uppsala University, 2021<br/><a href="http://uu.diva-portal.org/smash/record.jsf?pid=diva2%3A1565827&amp;dswid=61" target="_blank" rel="noopener noreferrer" class="text-blue-300 hover:underline">Thesis</a></p></div></section></div></main><script src="/_next/static/chunks/webpack-d6abbf1fcfdef890.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[6874,[\"177\",\"static/chunks/app/layout-21c00bc2bfe80b4e.js\"],\"\"]\n3:I[7555,[],\"\"]\n4:I[1295,[],\"\"]\n5:I[9665,[],\"OutletBoundary\"]\n8:I[9665,[],\"ViewportBoundary\"]\na:I[9665,[],\"MetadataBoundary\"]\nc:I[6614,[],\"\"]\n:HL[\"/_next/static/media/a34f9d1faa5f3315-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/css/31825d868cc34e7f.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"LDwzO5JUfJ-9tpKmkE8cM\",\"p\":\"\",\"c\":[\"\",\"research\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"research\",{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/31825d868cc34e7f.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"__className_d65c78\",\"children\":[[\"$\",\"nav\",null,{\"className\":\"bg-gray-800 text-white\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex items-center justify-between h-16\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex items-center\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex-shrink-0\",\"children\":[\"$\",\"span\",null,{\"className\":\"text-xl font-bold\",\"children\":\"Ziyang's Page\"}]}],[\"$\",\"div\",null,{\"className\":\"hidden md:block\",\"children\":[\"$\",\"div\",null,{\"className\":\"ml-10 flex items-baseline space-x-4\",\"children\":[[\"$\",\"$L2\",null,{\"href\":\"/\",\"className\":\"px-3 py-2 rounded-md text-sm font-medium hover:bg-gray-700\",\"children\":\"Home\"}],[\"$\",\"$L2\",null,{\"href\":\"/research\",\"className\":\"px-3 py-2 rounded-md text-sm font-medium hover:bg-gray-700\",\"children\":\"Research\"}],[\"$\",\"$L2\",null,{\"href\":\"/contact\",\"className\":\"px-3 py-2 rounded-md text-sm font-medium hover:bg-gray-700\",\"children\":\"Contact\"}]]}]}]]}]}]}]}],[\"$\",\"main\",null,{\"className\":\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8\",\"children\":[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"$undefined\",[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]]}]}]]}],{\"children\":[\"research\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"div\",null,{\"className\":\"space-y-12\",\"children\":[[\"$\",\"section\",null,{\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-3xl font-bold mb-6 text-white\",\"children\":\"Selected Publications\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-200 mb-4\",\"children\":\"(* equal contribution)\"}],[\"$\",\"div\",null,{\"className\":\"space-y-8\",\"children\":[[\"$\",\"div\",null,{\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mb-4 text-white\",\"children\":\"Preprints\"}],[\"$\",\"div\",null,{\"className\":\"space-y-6\",\"children\":[\"$\",\"div\",null,{\"className\":\"bg-gray-800/50 p-6 rounded-lg\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-gray-200 mb-2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"Aria-UI: Visual Grounding for GUI Instructions\"}],[\"$\",\"br\",null,{}],[\"$\",\"span\",null,{\"className\":\"text-gray-300\",\"children\":\"Yuhao Yang, Yue Wang, Dongxu Li, Ziyang Luo, Bei Chen, Chao Huang, and Junnan Li\"}],[\"$\",\"br\",null,{}],[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2412.16256\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Paper\"}]]}]}]}]]}],[\"$\",\"div\",null,{\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mb-4 text-white\",\"children\":\"2025\"}],[\"$\",\"div\",null,{\"className\":\"space-y-6\",\"children\":[[\"$\",\"div\",null,{\"className\":\"bg-gray-800/50 p-6 rounded-lg\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-gray-200 mb-2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"VideoAutoArena: An Automated Arena for Evaluating Large Multimodal Models in Video Analysis through User Simulation\"}],[\"$\",\"br\",null,{}],[\"$\",\"strong\",null,{\"children\":\"CVPR 2025\"}],\": Proceedings of the 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",[\"$\",\"br\",null,{}],[\"$\",\"span\",null,{\"className\":\"text-gray-300\",\"children\":\"Ziyang Luo, Haoning Wu, Dongxu Li, Jing Ma, Mohan Kankanhalli, and Junnan Li\"}],[\"$\",\"br\",null,{}],[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/pdf/2411.13281\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Paper\"}],\" | \",[\"$\",\"a\",null,{\"href\":\"https://videoautoarena.github.io/\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Project Page\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"bg-gray-800/50 p-6 rounded-lg\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-gray-200 mb-2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"ScreenSpot-Pro: GUI Grounding for Professional High-Resolution Computer Use\"}],[\"$\",\"br\",null,{}],[\"$\",\"strong\",null,{\"children\":\"ICLR 2025 Workshop\"}],\": Reasoning and Planning for LLMs Workshop\",[\"$\",\"br\",null,{}],[\"$\",\"span\",null,{\"className\":\"text-gray-300\",\"children\":\"Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang and Tat-Seng Chua\"}],[\"$\",\"br\",null,{}],[\"$\",\"a\",null,{\"href\":\"https://likaixin2000.github.io/papers/ScreenSpot_Pro.pdf\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Paper\"}],\" | \",[\"$\",\"a\",null,{\"href\":\"https://gui-agent.github.io/grounding-leaderboard/\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Project Page\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"bg-gray-800/50 p-6 rounded-lg\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-gray-200 mb-2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"ScratchEval: Are GPT-4o Smarter than My Child? Evaluating Large Multimodal Models with Visual Programming Challenges\"}],[\"$\",\"br\",null,{}],[\"$\",\"strong\",null,{\"children\":\"NAACL 2025\"}],\": Proceedings of the 2025 Annual Conference of the North American Chapter of the Association for Computational Linguistics\",[\"$\",\"br\",null,{}],[\"$\",\"span\",null,{\"className\":\"text-gray-300\",\"children\":\"Rao Fu, Ziyang Luo, Hongzhan Lin, Zhen Ye, Jing Ma\"}],[\"$\",\"br\",null,{}],[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2411.18932\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Paper\"}],\" | \",[\"$\",\"a\",null,{\"href\":\"https://github.com/HKBUNLP/ScratchEval\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Code\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"bg-gray-800/50 p-6 rounded-lg\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-gray-200 mb-2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"CodeHalu: Code Hallucinations in LLMs Driven by Execution-based Verification\"}],[\"$\",\"br\",null,{}],[\"$\",\"strong\",null,{\"children\":\"AAAI 2025\"}],\": Proceedings of the 39th Annual AAAI Conference on Artificial Intelligence\",[\"$\",\"br\",null,{}],[\"$\",\"span\",null,{\"className\":\"text-gray-300\",\"children\":\"Yuchen Tian, Weixiang Yan, Qian Yang, Xuandong Zhao, Qian Chen, Wen Wang, Ziyang Luo, Lei Ma, Dawn Song\"}],[\"$\",\"br\",null,{}],[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2405.00253\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Paper\"}],\" | \",[\"$\",\"a\",null,{\"href\":\"https://github.com/yuchen814/CodeHalu\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Code\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"bg-gray-800/50 p-6 rounded-lg\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-gray-200 mb-2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"CodeJudge-Eval: Can Large Language Models be Good Judges in Code Understanding?\"}],[\"$\",\"br\",null,{}],[\"$\",\"strong\",null,{\"children\":\"COLING 2025\"}],\": Proceedings of the 31st International Conference on Computational Linguistics\",[\"$\",\"br\",null,{}],[\"$\",\"span\",null,{\"className\":\"text-gray-300\",\"children\":\"Yuwei Zhao*, Ziyang Luo*, Yuchen Tian, Hongzhan Lin, Weixiang Yan, Annan Li, and Jing Ma\"}],[\"$\",\"br\",null,{}],[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2408.10718\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Paper\"}],\" | \",[\"$\",\"a\",null,{\"href\":\"https://huggingface.co/datasets/CodeResearch/CodeJudge-Eval\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Data\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"bg-gray-800/50 p-6 rounded-lg\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-gray-200 mb-2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"MFC-Bench: Benchmarking Multimodal Fact-Checking with Large Vision-Language Models\"}],[\"$\",\"br\",null,{}],[\"$\",\"strong\",null,{\"children\":\"ICLR 2025 Workshop\"}],\": Reasoning and Planning for LLMs Workshop\",[\"$\",\"br\",null,{}],[\"$\",\"span\",null,{\"className\":\"text-gray-300\",\"children\":\"Shengkang Wang*, Hongzhan Lin*, Ziyang Luo*, Zhen Ye, Guang Chen, and Jing Ma\"}],[\"$\",\"br\",null,{}],[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/pdf/2406.11288\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Paper\"}],\" | \",[\"$\",\"a\",null,{\"href\":\"https://github.com/wskbest/MFC-Bench\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Data\"}]]}]}]]}]]}],[\"$\",\"div\",null,{\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mb-4 text-white\",\"children\":\"2024\"}],[\"$\",\"div\",null,{\"className\":\"space-y-6\",\"children\":[[\"$\",\"div\",null,{\"className\":\"bg-gray-800/50 p-6 rounded-lg\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-gray-200 mb-2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"WizardCoder: Empowering Code Large Language Models with Evol-Instruct\"}],[\"$\",\"br\",null,{}],[\"$\",\"strong\",null,{\"children\":\"ICLR 2024\"}],\": Proceedings of the Twelfth International Conference on Learning Representations\",[\"$\",\"br\",null,{}],[\"$\",\"span\",null,{\"className\":\"text-gray-300\",\"children\":\"Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang\"}],[\"$\",\"br\",null,{}],[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2306.08568\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Paper\"}],\" | \",[\"$\",\"a\",null,{\"href\":\"https://huggingface.co/WizardLM/WizardCoder-15B-V1.0\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Model\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"bg-gray-800/50 p-6 rounded-lg\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-gray-200 mb-2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"AMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation\"}],[\"$\",\"br\",null,{}],[\"$\",\"strong\",null,{\"children\":\"EMNLP 2024\"}],\": Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing\",[\"$\",\"br\",null,{}],[\"$\",\"span\",null,{\"className\":\"text-gray-300\",\"children\":\"Ziyang Luo, Xin Li, Hongzhan Lin, Jing Ma, and Lidong Bing\"}],[\"$\",\"br\",null,{}],[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2410.00558\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Paper\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"bg-gray-800/50 p-6 rounded-lg\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-gray-200 mb-2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"Towards Low-Resource Harmful Meme Detection with LMM Agents\"}],[\"$\",\"br\",null,{}],[\"$\",\"strong\",null,{\"children\":\"EMNLP 2024\"}],\": Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing\",[\"$\",\"br\",null,{}],[\"$\",\"span\",null,{\"className\":\"text-gray-300\",\"children\":\"Jianzhao Huang, Hongzhan Lin, Ziyan Liu, Ziyang Luo, Guang Chen, and Jing Ma\"}],[\"$\",\"br\",null,{}],[\"$\",\"a\",null,{\"href\":\"https://aclanthology.org/2024.emnlp-main.136/\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Paper\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"bg-gray-800/50 p-6 rounded-lg\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-gray-200 mb-2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"MMCode: Evaluating Multi-Modal Code Large Language Models with Visually Rich Programming Problems\"}],[\"$\",\"br\",null,{}],[\"$\",\"strong\",null,{\"children\":\"EMNLP 2024\"}],\": Findings of the 2024 Conference on Empirical Methods in Natural Language Processing\",[\"$\",\"br\",null,{}],[\"$\",\"span\",null,{\"className\":\"text-gray-300\",\"children\":\"Kaixin Li, Yuchen Tian, Qisheng Hu, Ziyang Luo, and Jing Ma\"}],[\"$\",\"br\",null,{}],[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/pdf/2404.09486.pdf\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Paper\"}],\" | \",[\"$\",\"a\",null,{\"href\":\"https://huggingface.co/datasets/likaixin/MMCode\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Data\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"bg-gray-800/50 p-6 rounded-lg\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-gray-200 mb-2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"CofiPara: A Coarse-to-fine Paradigm for Multimodal Sarcasm Target Identification with Large Multimodal Models\"}],[\"$\",\"br\",null,{}],[\"$\",\"strong\",null,{\"children\":\"ACL 2024\"}],\": Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics\",[\"$\",\"br\",null,{}],[\"$\",\"span\",null,{\"className\":\"text-gray-300\",\"children\":\"Zixin Chen, Hongzhan Lin, Ziyang Luo, Mingfei Cheng, Jing Ma, and Guang Chen\"}],[\"$\",\"br\",null,{}],[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2405.00390\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Paper\"}],\" | \",[\"$\",\"a\",null,{\"href\":\"https://github.com/Lbotirx/CofiPara\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Code\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"bg-gray-800/50 p-6 rounded-lg\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-gray-200 mb-2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"Towards Explainable Harmful Meme Detection through Multimodal Debate between Large Language Models\"}],[\"$\",\"br\",null,{}],[\"$\",\"strong\",null,{\"children\":\"WWW 2024\"}],\": The ACM Web Conference 2024\",[\"$\",\"br\",null,{}],[\"$\",\"span\",null,{\"className\":\"text-gray-300\",\"children\":\"Hongzhan Lin, Ziyang Luo, Wei Gao, Jing Ma, Bo Wang, and Ruichao Yang\"}],[\"$\",\"br\",null,{}],[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2401.13298\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Paper\"}],\" | \",[\"$\",\"a\",null,{\"href\":\"https://github.com/HKBUNLP/ExplainHM-WWW2024\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Code\"}]]}]}]]}]]}],[\"$\",\"div\",null,{\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mb-4 text-white\",\"children\":\"2023\"}],[\"$\",\"div\",null,{\"className\":\"space-y-6\",\"children\":[[\"$\",\"div\",null,{\"className\":\"bg-gray-800/50 p-6 rounded-lg\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-gray-200 mb-2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"LexLIP: Lexicon-Bottlenecked Language-Image Pre-Training for Large-Scale Image-Text Sparse Retrieval\"}],[\"$\",\"br\",null,{}],[\"$\",\"strong\",null,{\"children\":\"ICCV 2023\"}],\": Proceedings of the IEEE/CVF International Conference on Computer Vision\",[\"$\",\"br\",null,{}],[\"$\",\"span\",null,{\"className\":\"text-gray-300\",\"children\":\"Ziyang Luo, Pu Zhao, Can Xu, Xiubo Geng, Tao Shen, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang\"}],[\"$\",\"br\",null,{}],[\"$\",\"a\",null,{\"href\":\"https://openaccess.thecvf.com/content/ICCV2023/papers/Luo_LexLIP_Lexicon-Bottlenecked_Language-Image_Pre-Training_for_Large-Scale_Image-Text_Sparse_Retrieval_ICCV_2023_paper.pdf\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Paper\"}],\" | \",[\"$\",\"a\",null,{\"href\":\"https://github.com/ChiYeungLaw/LexLIP-ICCV23\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Code\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"bg-gray-800/50 p-6 rounded-lg\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-gray-200 mb-2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"Beneath the Surface: Unveiling Harmful Memes with Multimodal Reasoning Distilled from Large Language Models\"}],[\"$\",\"br\",null,{}],[\"$\",\"strong\",null,{\"children\":\"EMNLP 2023\"}],\": Findings of the 2023 Conference on Empirical Methods in Natural Language Processing\",[\"$\",\"br\",null,{}],[\"$\",\"span\",null,{\"className\":\"text-gray-300\",\"children\":\"Hongzhan Lin*, Ziyang Luo*, Jing Ma, and Long Chen\"}],[\"$\",\"br\",null,{}],[\"$\",\"a\",null,{\"href\":\"https://aclanthology.org/2023.findings-emnlp.611/\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Paper\"}],\" | \",[\"$\",\"a\",null,{\"href\":\"https://github.com/HKBUNLP/Mr.Harm-EMNLP2023\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Code\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"bg-gray-800/50 p-6 rounded-lg\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-gray-200 mb-2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"Zero-Shot Rumor Detection with Propagation Structure via Prompt Learning\"}],[\"$\",\"br\",null,{}],[\"$\",\"strong\",null,{\"children\":\"AAAI 2023\"}],\": Proceedings of the 2023 AAAI Conference on Artificial Intelligence\",[\"$\",\"br\",null,{}],[\"$\",\"span\",null,{\"className\":\"text-gray-300\",\"children\":\"Hongzhan Lin, Pengyao Yi, Jing Ma, Haiyun Jiang, Ziyang Luo, Shuming Shi, Ruifang Liu\"}],[\"$\",\"br\",null,{}],[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2212.01117\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Paper\"}]]}]}]]}]]}],[\"$\",\"div\",null,{\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mb-4 text-white\",\"children\":\"2022\"}],[\"$\",\"div\",null,{\"className\":\"space-y-6\",\"children\":[[\"$\",\"div\",null,{\"className\":\"bg-gray-800/50 p-6 rounded-lg\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-gray-200 mb-2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"A Coarse-to-fine Cascaded Evidence-Distillation Neural Network for Explainable Fake News Detection\"}],[\"$\",\"br\",null,{}],[\"$\",\"strong\",null,{\"children\":\"COLING 2022\"}],\": Proceedings of the 29th International Conference on Computational Linguistics\",[\"$\",\"br\",null,{}],[\"$\",\"span\",null,{\"className\":\"text-gray-300\",\"children\":\"Zhiwei Yang, Jing Ma, Hechang Chen, Hongzhan Lin, Ziyang Luo, Yi Chang\"}],[\"$\",\"br\",null,{}],[\"$\",\"a\",null,{\"href\":\"https://aclanthology.org/2022.coling-1.230/\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Paper\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"bg-gray-800/50 p-6 rounded-lg\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-gray-200 mb-2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"Conditioned Masked Language and Image Modeling for Image-Text Dense Retrieval\"}],[\"$\",\"br\",null,{}],[\"$\",\"strong\",null,{\"children\":\"EMNLP 2022\"}],\": Findings of the 2022 Conference on Empirical Methods in Natural Language Processing\",[\"$\",\"br\",null,{}],[\"$\",\"span\",null,{\"className\":\"text-gray-300\",\"children\":\"Ziyang Luo, Yadong Xi, Rongsheng Zhang, Gongzheng Li, Zeng Zhao, and Jing Ma\"}],[\"$\",\"br\",null,{}],[\"$\",\"a\",null,{\"href\":\"https://aclanthology.org/2022.findings-emnlp.10/\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Paper\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"bg-gray-800/50 p-6 rounded-lg\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-gray-200 mb-2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"DecBERT: Enhancing the Language Understanding of BERT with Causal Attention Masks\"}],[\"$\",\"br\",null,{}],[\"$\",\"strong\",null,{\"children\":\"NAACL 2022\"}],\": Findings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics\",[\"$\",\"br\",null,{}],[\"$\",\"span\",null,{\"className\":\"text-gray-300\",\"children\":\"Ziyang Luo, Yadong Xi, Jing Ma, Zhiwei Yang, Xiaoxi Mao, Changjie Fan, Rongsheng Zhang\"}],[\"$\",\"br\",null,{}],[\"$\",\"a\",null,{\"href\":\"https://aclanthology.org/2022.findings-naacl.89/\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Paper\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"bg-gray-800/50 p-6 rounded-lg\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-gray-200 mb-2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"Easy and Efficient Transformer: Scalable Inference Solution for Large NLP Model\"}],[\"$\",\"br\",null,{}],[\"$\",\"strong\",null,{\"children\":\"NAACL 2022\"}],\": Industry Track\",[\"$\",\"br\",null,{}],[\"$\",\"span\",null,{\"className\":\"text-gray-300\",\"children\":\"Yadong Xi, Gongzheng Li, Jingzhen Ding, Duan Wang, Ziyang Luo, Rongsheng Zhang, Bai Liu, Changjie Fan, Xiaoxi Mao, Zeng Zhao\"}],[\"$\",\"br\",null,{}],[\"$\",\"a\",null,{\"href\":\"https://aclanthology.org/2022.naacl-industry.8/\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Paper\"}]]}]}]]}]]}],[\"$\",\"div\",null,{\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mb-4 text-white\",\"children\":\"2021\"}],[\"$\",\"div\",null,{\"className\":\"space-y-6\",\"children\":[[\"$\",\"div\",null,{\"className\":\"bg-gray-800/50 p-6 rounded-lg\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-gray-200 mb-2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"Positional Artefacts Propagate Through Masked Language Model Embeddings\"}],[\"$\",\"br\",null,{}],[\"$\",\"strong\",null,{\"children\":\"ACL 2021\"}],\": Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\",[\"$\",\"br\",null,{}],[\"$\",\"span\",null,{\"className\":\"text-gray-300\",\"children\":\"Ziyang Luo, Artur Kulmizev, and Xiaoxi Mao\"}],[\"$\",\"br\",null,{}],[\"$\",\"a\",null,{\"href\":\"https://aclanthology.org/2021.acl-long.413/\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Paper\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"bg-gray-800/50 p-6 rounded-lg\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-gray-200 mb-2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"Smoothing with Fake Label\"}],[\"$\",\"br\",null,{}],[\"$\",\"strong\",null,{\"children\":\"CIKM 2021\"}],\": Proceedings of the 30th ACM International Conference on Information and Knowledge Management\",[\"$\",\"br\",null,{}],[\"$\",\"span\",null,{\"className\":\"text-gray-300\",\"children\":\"Ziyang Luo, Yadong Xi, and Xiaoxi Mao\"}],[\"$\",\"br\",null,{}],[\"$\",\"a\",null,{\"href\":\"/research/Fake_Label_CIKM.pdf\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Paper\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"bg-gray-800/50 p-6 rounded-lg\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-gray-200 mb-2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"Gender Bias Hidden Behind Chinese Word Embeddings: The Case of Chinese Adjectives\"}],[\"$\",\"br\",null,{}],[\"$\",\"strong\",null,{\"children\":\"GeBNLP 2021\"}],\": Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing\",[\"$\",\"br\",null,{}],[\"$\",\"span\",null,{\"className\":\"text-gray-300\",\"children\":\"Meichun Jiao, Ziyang Luo\"}],[\"$\",\"br\",null,{}],[\"$\",\"a\",null,{\"href\":\"https://aclanthology.org/2021.gebnlp-1.2/\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Paper\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"bg-gray-800/50 p-6 rounded-lg\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-gray-200 mb-2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"Have Attention Heads in BERT Learned Constituency Grammar?\"}],[\"$\",\"br\",null,{}],[\"$\",\"strong\",null,{\"children\":\"EACL 2021 SRW\"}],\": Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop\",[\"$\",\"br\",null,{}],[\"$\",\"span\",null,{\"className\":\"text-gray-300\",\"children\":\"Ziyang Luo\"}],[\"$\",\"br\",null,{}],[\"$\",\"a\",null,{\"href\":\"https://aclanthology.org/2021.eacl-srw.2/\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Paper\"}]]}]}]]}]]}]]}]]}],[\"$\",\"section\",null,{\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-2xl font-bold mb-6 text-white\",\"children\":\"Selected Open-Source Projects\"}],[\"$\",\"div\",null,{\"className\":\"space-y-6\",\"children\":[[\"$\",\"div\",null,{\"className\":\"bg-gray-800/50 p-6 rounded-lg\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-gray-200\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs\"}],[\"$\",\"br\",null,{}],\"VideoLLaMA Team@Alibaba\",[\"$\",\"br\",null,{}],[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/pdf/2406.07476\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Paper\"}],\" | \",[\"$\",\"a\",null,{\"href\":\"https://github.com/DAMO-NLP-SG/VideoLLaMA2\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Code\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"bg-gray-800/50 p-6 rounded-lg\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-gray-200\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"WizardCoder: Empowering Code Large Language Models with Evol-Instruct\"}],[\"$\",\"br\",null,{}],\"WizardLM Team@Microsoft\",[\"$\",\"br\",null,{}],[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2306.08568\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Paper\"}],\" | \",[\"$\",\"a\",null,{\"href\":\"https://github.com/nlpxucan/WizardLM\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Code\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"bg-gray-800/50 p-6 rounded-lg\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-gray-200\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"AURORA-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order\"}],[\"$\",\"br\",null,{}],\"Aurora-M Open-Source Community\",[\"$\",\"br\",null,{}],[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/pdf/2404.00399.pdf\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Paper\"}],\" | \",[\"$\",\"a\",null,{\"href\":\"https://huggingface.co/collections/aurora-m/aurora-m-models-65fdfdff62471e09812f5407\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Models\"}]]}]}]]}]]}],[\"$\",\"section\",null,{\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-2xl font-bold mb-6 text-white\",\"children\":\"Master Thesis\"}],[\"$\",\"div\",null,{\"className\":\"bg-gray-800/50 p-6 rounded-lg\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-gray-200\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"Analyzing the Anisotropy Phenomenon in Transformer-based Masked Language Models\"}],[\"$\",\"br\",null,{}],\"Supervised by \",[\"$\",\"a\",null,{\"href\":\"https://scholar.google.nl/citations?hl=en\u0026user=Cgg6_W0AAAAJ\u0026view_op=list_works\u0026sortby=pubdate\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Artur Kulmizev\"}],\", Uppsala University, 2021\",[\"$\",\"br\",null,{}],[\"$\",\"a\",null,{\"href\":\"http://uu.diva-portal.org/smash/record.jsf?pid=diva2%3A1565827\u0026dswid=61\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-blue-300 hover:underline\",\"children\":\"Thesis\"}]]}]}]]}]]}],\"$undefined\",null,[\"$\",\"$L5\",null,{\"children\":[\"$L6\",\"$L7\",null]}]]}],{},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"EgbXTk_mcEiYOFA0XdtCM\",{\"children\":[[\"$\",\"$L8\",null,{\"children\":\"$L9\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}],[\"$\",\"$La\",null,{\"children\":\"$Lb\"}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$c\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"9:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n6:null\n"])</script><script>self.__next_f.push([1,"7:null\nb:[[\"$\",\"title\",\"0\",{\"children\":\"Ziyang\u0026apos;s Page\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Welcome to Ziyang Luo's personal website\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"link\",\"3\",{\"rel\":\"icon\",\"href\":\"/images/me.jpg\"}],[\"$\",\"link\",\"4\",{\"rel\":\"apple-touch-icon\",\"href\":\"/images/me.jpg\"}]]\n"])</script></body></html>